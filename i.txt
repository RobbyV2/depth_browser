# Role: Senior WebXR & Systems Engineer
# Project: DepthXR-Browser
# Context: Rust (Axum), Next.js, WebXR (R3F), WebGPU, PyO3

**Current State:**
- The application successfully captures a screen/window 
- This stream is rendering to a standard `<video>` element (hidden or visible) in the DOM.
- We are ready to implement the Depth Estimation pipeline.

**Objective:**
Implement a dual-mode depth pipeline (Client-Side WebGPU vs. Server-Side PyO3) that reads from this live video element and renders a depth-displaced 3D mesh in WebXR.

## Implementation Phases (Strict Order)
**Do NOT implement all phases at once.** Execute Phase 3, verify benchmarks, and then proceed to Phase 4.

### Phase 3: Client-Side Depth (WebGPU / Transformers.js)
**Goal:** Run Depth Anything V2 Small locally on the video stream.

1.  **Setup:**
    - Install `@xenova/transformers`.
    - Configure the pipeline to use the `webgpu` device and `fp16` dtype (critical for speed).
    - Load `Xenova/depth-anything-v2-small`.

2.  **The Loop (RequestAnimationFrame):**
    - Create a loop that grabs the current frame from the `<video>` element.
    - Pass it to the `depth-estimation` pipeline.
    - **Optimization:** Do not run inference every single frame if it drops below 30FPS. Implement a `throttle` mechanism (e.g., run depth every 2nd or 3rd frame, interpolate the result).

3.  **Visualization (R3F):**
    - Create a `Mesh` (PlaneGeometry) with a high segment count (e.g., 128x128).
    - **Texture A (Color):** Bind the `<video>` element as a `VideoTexture` to the `map`.
    - **Texture B (Displacement):** Bind the inference output (Canvas/DataTexture) to the `displacementMap`.

4.  **Benchmark:**
    - Log the inference time (ms) to the console: `[BENCH-GPU] Inference: 15ms`.
    - Verify visual synchronization between the video movement and the depth movement.

### Phase 4: Server-Side Depth (Rust/Axum + PyO3 + WebSockets)
**Goal:** Offload heavy inference (DA3) to the server using a persistent WebSocket connection (HTTP POST is too slow for video).

1.  **Python Setup (PyO3):**
    - Ensure the Python environment has `torch`, `depth-anything`, and `cv2` (or `pillow`).
    - **Global State:** The Python model must be loaded **once** at server startup and held in memory (Mutex/Arc), not reloaded per request.

2.  **Rust Backend (WebSockets):**
    - Create a route `GET /ws/depth`.
    - **Protocol:**
        - **In:** Binary message (JPEG encoded frame).
        - **Processing:** Rust sends bytes to PyO3 -> Python runs inference -> Returns generic Gray8 bytes.
        - **Out:** Binary message (Grayscale Depth buffer).
    - **Concurrency:** Ensure the Python GIL does not block the WebSocket heartbeat. Use `tokio::task::spawn_blocking`.

3.  **Frontend Integration:**
    - Add a toggle: "Mode: Server (High Quality)".
    - When active:
        - Draw video frame to an offscreen canvas.
        - `canvas.toBlob('image/jpeg', 0.7)` -> Send via WebSocket.
        - On Message -> Update the `displacementMap` texture.

4.  **Benchmark:**
    - Measure `RTT` (Round Trip Time): Time from sending frame to receiving depth.
    - Target: < 100ms.

### Phase 5: The "XR Cinema" Polish
**Goal:** creating a comfortable viewing experience.

1.  **Shader Material:**
    - Replace the standard `MeshStandardMaterial` with a custom `ShaderMaterial`.
    - **Why:** Standard displacement creates jagged spikes on noise.
    - **Logic:** Implement a simple "Depth Smooth" shader that mixes the previous frame's depth with the new one (temporal smoothing) to reduce flickering.

2.  **UI Controls:**
    - Add a "Depth Scale" slider (0.0 to 1.0) to control the intensity of the 3D effect.
    - Add a "Zero Plane" slider to control *where* the screen sits in 3D space (does it pop out or push in?).

## Coding Guidelines
- **Type Safety:** Use rigid types for the PyO3 interactions (e.g., define a Rust struct for the Image Tensor wrapper).
- **Error Handling:** If the WebSocket disconnects, automatically fallback to Client-Side mode.
- **Performance:** In Phase 3, ensure `tensor.dispose()` is called if manual memory management is required by the library version used.

## START INSTRUCTION
Begin with **Phase 3**. Set up the `@xenova/transformers` pipeline with WebGPU support and log the inference time for a static test image before hooking up the video loop.